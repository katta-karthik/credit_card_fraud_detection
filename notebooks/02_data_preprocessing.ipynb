{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5569c6d0",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Data Preprocessing\n",
    "\n",
    "This notebook handles data preprocessing including:\n",
    "- Data cleaning and preparation\n",
    "- Feature engineering\n",
    "- Handling class imbalance with SMOTE\n",
    "- Train-test split preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2973a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cea1c",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1d778f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 243. MiB for an array with shape (5, 6362620) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      3\u001b[39m     data_path = \u001b[33m\"\u001b[39m\u001b[33m../data/raw/PS_20174392719_1491204439457_log.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset loaded successfully from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2144\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2142\u001b[39m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[32m0\u001b[39m].shape, axes, e)\n\u001b[32m   2143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[32m-> \u001b[39m\u001b[32m2144\u001b[39m     \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\katta\\Desktop\\Fraud Detection\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2298\u001b[39m     new_values = bvals2[\u001b[32m0\u001b[39m]._concat_same_type(bvals2, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   2300\u001b[39m argsort = np.argsort(new_mgr_locs)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m new_values = \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2302\u001b[39m new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m   2304\u001b[39m bp = BlockPlacement(new_mgr_locs)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 243. MiB for an array with shape (5, 6362620) and data type float64"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data_path = \"../data/raw/PS_20174392719_1491204439457_log.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset loaded successfully from {data_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset file not found!\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 10000\n",
    "    \n",
    "    sample_data = {\n",
    "        'step': np.random.randint(1, 744, n_samples),\n",
    "        'type': np.random.choice(['CASH_OUT', 'PAYMENT', 'CASH_IN', 'TRANSFER', 'DEBIT'], n_samples),\n",
    "        'amount': np.random.exponential(100, n_samples),\n",
    "        'nameOrig': [f'C{i}' for i in range(n_samples)],\n",
    "        'oldbalanceOrg': np.random.exponential(1000, n_samples),\n",
    "        'newbalanceOrig': np.random.exponential(1000, n_samples),\n",
    "        'nameDest': [f'M{i}' for i in range(n_samples)],\n",
    "        'oldbalanceDest': np.random.exponential(1000, n_samples),\n",
    "        'newbalanceDest': np.random.exponential(1000, n_samples),\n",
    "        'isFraud': np.random.choice([0, 1], n_samples, p=[0.998, 0.002])\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"Sample data created with shape: {df.shape}\")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed8d38",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0091ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if df.duplicated().sum() > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Duplicates removed. New shape: {df.shape}\")\n",
    "\n",
    "# Basic data cleaning\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for any anomalies\n",
    "print(f\"\\nNegative amounts: {(df['amount'] < 0).sum()}\")\n",
    "print(f\"Zero amounts: {(df['amount'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0684843",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf495596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_processed = df.copy()\n",
    "\n",
    "# 1. Balance difference features\n",
    "df_processed['balance_diff_orig'] = df_processed['oldbalanceOrg'] - df_processed['newbalanceOrig']\n",
    "df_processed['balance_diff_dest'] = df_processed['newbalanceDest'] - df_processed['oldbalanceDest']\n",
    "\n",
    "# 2. Balance ratio features\n",
    "df_processed['orig_balance_ratio'] = df_processed['newbalanceOrig'] / (df_processed['oldbalanceOrg'] + 1)\n",
    "df_processed['dest_balance_ratio'] = df_processed['newbalanceDest'] / (df_processed['oldbalanceDest'] + 1)\n",
    "\n",
    "# 3. Amount vs balance ratios\n",
    "df_processed['amount_to_orig_ratio'] = df_processed['amount'] / (df_processed['oldbalanceOrg'] + 1)\n",
    "df_processed['amount_to_dest_ratio'] = df_processed['amount'] / (df_processed['oldbalanceDest'] + 1)\n",
    "\n",
    "# 4. Log transformed features\n",
    "df_processed['log_amount'] = np.log1p(df_processed['amount'])\n",
    "df_processed['log_oldbalanceOrg'] = np.log1p(df_processed['oldbalanceOrg'])\n",
    "df_processed['log_newbalanceOrig'] = np.log1p(df_processed['newbalanceOrig'])\n",
    "df_processed['log_oldbalanceDest'] = np.log1p(df_processed['oldbalanceDest'])\n",
    "df_processed['log_newbalanceDest'] = np.log1p(df_processed['newbalanceDest'])\n",
    "\n",
    "# 5. Transaction type encoding\n",
    "le_type = LabelEncoder()\n",
    "df_processed['type_encoded'] = le_type.fit_transform(df_processed['type'])\n",
    "\n",
    "# 6. Categorical features - One hot encoding\n",
    "type_dummies = pd.get_dummies(df_processed['type'], prefix='type')\n",
    "df_processed = pd.concat([df_processed, type_dummies], axis=1)\n",
    "\n",
    "# 7. Binary flags\n",
    "df_processed['is_orig_zero_balance'] = (df_processed['oldbalanceOrg'] == 0).astype(int)\n",
    "df_processed['is_dest_zero_balance'] = (df_processed['oldbalanceDest'] == 0).astype(int)\n",
    "df_processed['is_amount_round'] = (df_processed['amount'] % 1 == 0).astype(int)\n",
    "\n",
    "print(f\"Feature engineering completed.\")\n",
    "print(f\"New shape: {df_processed.shape}\")\n",
    "print(f\"New features created: {df_processed.shape[1] - df.shape[1]}\")\n",
    "\n",
    "# Display new features\n",
    "new_features = [col for col in df_processed.columns if col not in df.columns]\n",
    "print(f\"\\nNew features: {new_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114aae2",
   "metadata": {},
   "source": [
    "## Feature Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "# Exclude ID columns and original categorical column\n",
    "exclude_cols = ['nameOrig', 'nameDest', 'type']\n",
    "feature_cols = [col for col in df_processed.columns if col not in exclude_cols + ['isFraud']]\n",
    "\n",
    "X = df_processed[feature_cols]\n",
    "y = df_processed['isFraud']\n",
    "\n",
    "print(f\"Features for modeling: {len(feature_cols)}\")\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "# Check for any infinite or NaN values\n",
    "print(f\"\\nInfinite values: {np.isinf(X).sum().sum()}\")\n",
    "print(f\"NaN values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Replace infinite values with NaN and then fill\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"\\nFinal X shape: {X.shape}\")\n",
    "print(f\"Final y shape: {y.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67548b8f",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e21c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Calculate class distribution percentages\n",
    "train_fraud_rate = y_train.sum() / len(y_train) * 100\n",
    "test_fraud_rate = y_test.sum() / len(y_test) * 100\n",
    "\n",
    "print(f\"\\nTrain fraud rate: {train_fraud_rate:.3f}%\")\n",
    "print(f\"Test fraud rate: {test_fraud_rate:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3092b1",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"Features scaled successfully\")\n",
    "print(f\"Scaled train mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"Scaled train std: {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "# Show scaling statistics\n",
    "print(f\"\\nScaling verification:\")\n",
    "print(f\"Train set - Mean range: [{X_train_scaled.mean().min():.6f}, {X_train_scaled.mean().max():.6f}]\")\n",
    "print(f\"Train set - Std range: [{X_train_scaled.std().min():.6f}, {X_train_scaled.std().max():.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5781cf4",
   "metadata": {},
   "source": [
    "## Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to handle class imbalance\n",
    "print(\"Applying SMOTE to handle class imbalance...\")\n",
    "\n",
    "# Before SMOTE\n",
    "print(f\"Before SMOTE:\")\n",
    "print(f\"Class distribution: {y_train.value_counts()}\")\n",
    "print(f\"Class ratios: {y_train.value_counts(normalize=True)}\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_smote = pd.DataFrame(X_train_smote, columns=X_train_scaled.columns)\n",
    "y_train_smote = pd.Series(y_train_smote)\n",
    "\n",
    "# After SMOTE\n",
    "print(f\"\\nAfter SMOTE:\")\n",
    "print(f\"Class distribution: {y_train_smote.value_counts()}\")\n",
    "print(f\"Class ratios: {y_train_smote.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTraining set size increased from {len(y_train)} to {len(y_train_smote)}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "y_train.value_counts().plot(kind='bar', ax=ax1, color=['blue', 'red'])\n",
    "ax1.set_title('Class Distribution - Before SMOTE')\n",
    "ax1.set_xlabel('Class')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "\n",
    "# After SMOTE\n",
    "y_train_smote.value_counts().plot(kind='bar', ax=ax2, color=['blue', 'red'])\n",
    "ax2.set_title('Class Distribution - After SMOTE')\n",
    "ax2.set_xlabel('Class')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f5b12",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ca09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed data directory\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "datasets = {\n",
    "    'X_train_original': X_train_scaled,\n",
    "    'X_train_smote': X_train_smote,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train_original': y_train,\n",
    "    'y_train_smote': y_train_smote,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    filepath = processed_dir / f\"{name}.pkl\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved {name} to {filepath}\")\n",
    "\n",
    "# Save scaler and label encoder\n",
    "with open(processed_dir / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "    \n",
    "with open(processed_dir / 'label_encoder_type.pkl', 'wb') as f:\n",
    "    pickle.dump(le_type, f)\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    'feature_columns': feature_cols,\n",
    "    'all_columns': list(df_processed.columns),\n",
    "    'original_columns': list(df.columns)\n",
    "}\n",
    "\n",
    "with open(processed_dir / 'feature_info.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "\n",
    "print(\"\\nAll preprocessing artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d4ed1",
   "metadata": {},
   "source": [
    "## Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final preprocessing summary\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Original shape: {df.shape}\")\n",
    "print(f\"   - Final shape: {df_processed.shape}\")\n",
    "print(f\"   - Features created: {df_processed.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n2. FEATURE ENGINEERING:\")\n",
    "print(f\"   - Balance difference features: 2\")\n",
    "print(f\"   - Balance ratio features: 2\")\n",
    "print(f\"   - Amount ratio features: 2\")\n",
    "print(f\"   - Log transformed features: 5\")\n",
    "print(f\"   - One-hot encoded features: {len([col for col in df_processed.columns if col.startswith('type_')])}\")\n",
    "print(f\"   - Binary flag features: 3\")\n",
    "\n",
    "print(f\"\\n3. DATA SPLITS:\")\n",
    "print(f\"   - Training set: {X_train.shape}\")\n",
    "print(f\"   - Test set: {X_test.shape}\")\n",
    "print(f\"   - Training set (after SMOTE): {X_train_smote.shape}\")\n",
    "\n",
    "print(f\"\\n4. CLASS DISTRIBUTION:\")\n",
    "print(f\"   - Original train fraud rate: {train_fraud_rate:.3f}%\")\n",
    "print(f\"   - SMOTE train fraud rate: {(y_train_smote.sum() / len(y_train_smote)) * 100:.1f}%\")\n",
    "print(f\"   - Test fraud rate: {test_fraud_rate:.3f}%\")\n",
    "\n",
    "print(f\"\\n5. PREPROCESSING STEPS COMPLETED:\")\n",
    "print(f\"   ✓ Data cleaning and validation\")\n",
    "print(f\"   ✓ Feature engineering\")\n",
    "print(f\"   ✓ Feature scaling (StandardScaler)\")\n",
    "print(f\"   ✓ Train-test split (stratified)\")\n",
    "print(f\"   ✓ Class imbalance handling (SMOTE)\")\n",
    "print(f\"   ✓ Data persistence\")\n",
    "\n",
    "print(f\"\\n6. READY FOR MODELING:\")\n",
    "print(f\"   - Features available: {len(feature_cols)}\")\n",
    "print(f\"   - Balanced dataset for training\")\n",
    "print(f\"   - Preserved test set for evaluation\")\n",
    "print(f\"   - All preprocessing artifacts saved\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETED - Ready for model training\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
