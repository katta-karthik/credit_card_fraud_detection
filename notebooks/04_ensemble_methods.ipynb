{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab142eff",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Ensemble Methods\n",
    "\n",
    "This notebook implements ensemble learning techniques to improve fraud detection performance:\n",
    "- Voting Classifier (Hard and Soft Voting)\n",
    "- Stacking Classifier\n",
    "- Model combination strategies\n",
    "- Performance comparison with individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, VotingClassifier, \n",
    "    StackingClassifier, AdaBoostClassifier,\n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9bfda",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data and Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "processed_dir = Path('../data/processed')\n",
    "models_dir = Path('../models')\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load preprocessed datasets\"\"\"\n",
    "    datasets = {}\n",
    "    files_to_load = [\n",
    "        'X_train_smote', 'X_test', 'y_train_smote', 'y_test'\n",
    "    ]\n",
    "    \n",
    "    for filename in files_to_load:\n",
    "        try:\n",
    "            with open(processed_dir / f\"{filename}.pkl\", 'rb') as f:\n",
    "                datasets[filename] = pickle.load(f)\n",
    "            print(f\"Loaded {filename}: {datasets[filename].shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {filename}.pkl not found\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def load_trained_models():\n",
    "    \"\"\"Load pre-trained individual models\"\"\"\n",
    "    models = {}\n",
    "    model_files = ['best_knn', 'best_svm', 'best_dt', 'best_rf']\n",
    "    \n",
    "    for model_name in model_files:\n",
    "        try:\n",
    "            with open(models_dir / f\"{model_name}.pkl\", 'rb') as f:\n",
    "                models[model_name] = pickle.load(f)\n",
    "            print(f\"Loaded {model_name}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {model_name}.pkl not found\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Load data\n",
    "data = load_data()\n",
    "trained_models = load_trained_models()\n",
    "\n",
    "if data:\n",
    "    X_train_smote = data['X_train_smote']\n",
    "    X_test = data['X_test']\n",
    "    y_train_smote = data['y_train_smote']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    print(f\"\\nData loaded successfully!\")\n",
    "    print(f\"Training samples: {X_train_smote.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"Features: {X_train_smote.shape[1]}\")\n",
    "else:\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    n_samples = 1000\n",
    "    n_features = 20\n",
    "    \n",
    "    X_train_smote = pd.DataFrame(np.random.randn(n_samples, n_features))\n",
    "    X_test = pd.DataFrame(np.random.randn(200, n_features))\n",
    "    y_train_smote = pd.Series(np.random.choice([0, 1], n_samples))\n",
    "    y_test = pd.Series(np.random.choice([0, 1], 200, p=[0.8, 0.2]))\n",
    "    \n",
    "    trained_models = {}\n",
    "    print(f\"Sample data created for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac8a6d",
   "metadata": {},
   "source": [
    "## Model Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"Comprehensive ensemble model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Record training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # ROC AUC\n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = \"N/A\"\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Cross-validation score\n",
    "    try:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='f1')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "    except:\n",
    "        cv_mean = cv_std = \"N/A\"\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Training Time: {training_time:.3f} seconds\")\n",
    "    print(f\"Prediction Time: {prediction_time:.3f} seconds\")\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"ROC AUC:   {roc_auc if roc_auc != 'N/A' else 'N/A'}\")\n",
    "    \n",
    "    if cv_mean != \"N/A\":\n",
    "        print(f\"CV F1 Score: {cv_mean:.4f} (+/- {cv_std * 2:.4f})\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'training_time': training_time,\n",
    "        'prediction_time': prediction_time,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Legitimate', 'Fraud'],\n",
    "                yticklabels=['Legitimate', 'Fraud'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Ensemble evaluation framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfcf8c",
   "metadata": {},
   "source": [
    "## Base Models for Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41520e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base models for ensemble\n",
    "if trained_models:\n",
    "    # Use pre-trained optimized models\n",
    "    base_models = [\n",
    "        ('knn', trained_models.get('best_knn', KNeighborsClassifier(n_neighbors=5))),\n",
    "        ('svm', trained_models.get('best_svm', SVC(probability=True, random_state=42))),\n",
    "        ('dt', trained_models.get('best_dt', DecisionTreeClassifier(random_state=42))),\n",
    "        ('rf', trained_models.get('best_rf', RandomForestClassifier(random_state=42)))\n",
    "    ]\n",
    "    print(\"Using pre-trained optimized models\")\n",
    "else:\n",
    "    # Create new base models with good default parameters\n",
    "    base_models = [\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5, weights='distance')),\n",
    "        ('svm', SVC(kernel='rbf', C=1.0, probability=True, random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(max_depth=10, min_samples_split=10, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1))\n",
    "    ]\n",
    "    print(\"Using new base models with default parameters\")\n",
    "\n",
    "print(f\"Base models for ensemble: {[name for name, _ in base_models]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927eab27",
   "metadata": {},
   "source": [
    "## 1. Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a83e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Voting Classifier\n",
    "print(\"Training Hard Voting Classifier...\")\n",
    "\n",
    "hard_voting = VotingClassifier(\n",
    "    estimators=base_models,\n",
    "    voting='hard',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "hard_voting_results = evaluate_ensemble(\n",
    "    hard_voting, X_train_smote, y_train_smote, X_test, y_test, \"Hard Voting Classifier\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(hard_voting_results['confusion_matrix'], \"Hard Voting Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting Classifier\n",
    "print(\"\\nTraining Soft Voting Classifier...\")\n",
    "\n",
    "soft_voting = VotingClassifier(\n",
    "    estimators=base_models,\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "soft_voting_results = evaluate_ensemble(\n",
    "    soft_voting, X_train_smote, y_train_smote, X_test, y_test, \"Soft Voting Classifier\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(soft_voting_results['confusion_matrix'], \"Soft Voting Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37227ec5",
   "metadata": {},
   "source": [
    "## 2. Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier with Logistic Regression as meta-learner\n",
    "print(\"\\nTraining Stacking Classifier...\")\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(random_state=42),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_results = evaluate_ensemble(\n",
    "    stacking, X_train_smote, y_train_smote, X_test, y_test, \"Stacking Classifier (LR)\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(stacking_results['confusion_matrix'], \"Stacking Classifier (LR)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c54d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier with Random Forest as meta-learner\n",
    "print(\"\\nTraining Stacking Classifier with RF Meta-learner...\")\n",
    "\n",
    "stacking_rf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_rf_results = evaluate_ensemble(\n",
    "    stacking_rf, X_train_smote, y_train_smote, X_test, y_test, \"Stacking Classifier (RF)\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(stacking_rf_results['confusion_matrix'], \"Stacking Classifier (RF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c4832",
   "metadata": {},
   "source": [
    "## 3. Additional Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1769f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "print(\"\\nTraining AdaBoost Classifier...\")\n",
    "\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada_boost_results = evaluate_ensemble(\n",
    "    ada_boost, X_train_smote, y_train_smote, X_test, y_test, \"AdaBoost Classifier\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(ada_boost_results['confusion_matrix'], \"AdaBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f03948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "print(\"\\nTraining Gradient Boosting Classifier...\")\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_results = evaluate_ensemble(\n",
    "    gb_classifier, X_train_smote, y_train_smote, X_test, y_test, \"Gradient Boosting\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(gb_results['confusion_matrix'], \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be42cf",
   "metadata": {},
   "source": [
    "## 4. Weighted Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Voting based on individual model performance\n",
    "print(\"\\nTraining Weighted Voting Classifier...\")\n",
    "\n",
    "# Assign weights based on expected performance (higher weight for better models)\n",
    "# In practice, these would be determined from validation performance\n",
    "model_weights = [1.0, 1.2, 1.1, 1.3]  # knn, svm, dt, rf\n",
    "\n",
    "weighted_voting = VotingClassifier(\n",
    "    estimators=base_models,\n",
    "    voting='soft',\n",
    "    weights=model_weights,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "weighted_voting_results = evaluate_ensemble(\n",
    "    weighted_voting, X_train_smote, y_train_smote, X_test, y_test, \"Weighted Voting Classifier\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(weighted_voting_results['confusion_matrix'], \"Weighted Voting Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546036da",
   "metadata": {},
   "source": [
    "## Ensemble Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all ensemble results\n",
    "ensemble_results = [\n",
    "    hard_voting_results,\n",
    "    soft_voting_results,\n",
    "    stacking_results,\n",
    "    stacking_rf_results,\n",
    "    ada_boost_results,\n",
    "    gb_results,\n",
    "    weighted_voting_results\n",
    "]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "ensemble_comparison_data = []\n",
    "for result in ensemble_results:\n",
    "    ensemble_comparison_data.append({\n",
    "        'Model': result['model_name'],\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1 Score': result['f1_score'],\n",
    "        'ROC AUC': result['roc_auc'] if result['roc_auc'] != \"N/A\" else np.nan,\n",
    "        'CV F1 Mean': result['cv_mean'] if result['cv_mean'] != \"N/A\" else np.nan,\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Prediction Time (s)': result['prediction_time']\n",
    "    })\n",
    "\n",
    "ensemble_comparison_df = pd.DataFrame(ensemble_comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ENSEMBLE METHODS COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(ensemble_comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Find best ensemble model\n",
    "best_ensemble_idx = ensemble_comparison_df['F1 Score'].idxmax()\n",
    "best_ensemble_name = ensemble_comparison_df.loc[best_ensemble_idx, 'Model']\n",
    "best_ensemble_f1 = ensemble_comparison_df.loc[best_ensemble_idx, 'F1 Score']\n",
    "\n",
    "print(f\"\\nBest Ensemble Model: {best_ensemble_name} (F1 Score: {best_ensemble_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of ensemble comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# F1 Score comparison\n",
    "sns.barplot(data=ensemble_comparison_df, x='F1 Score', y='Model', ax=axes[0,0])\n",
    "axes[0,0].set_title('Ensemble Models F1 Score Comparison')\n",
    "\n",
    "# Precision vs Recall\n",
    "axes[0,1].scatter(ensemble_comparison_df['Precision'], ensemble_comparison_df['Recall'], s=100)\n",
    "for i, model in enumerate(ensemble_comparison_df['Model']):\n",
    "    axes[0,1].annotate(model, \n",
    "                      (ensemble_comparison_df['Precision'].iloc[i], ensemble_comparison_df['Recall'].iloc[i]),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0,1].set_xlabel('Precision')\n",
    "axes[0,1].set_ylabel('Recall')\n",
    "axes[0,1].set_title('Precision vs Recall - Ensemble Methods')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC AUC comparison\n",
    "roc_data = ensemble_comparison_df.dropna(subset=['ROC AUC'])\n",
    "if not roc_data.empty:\n",
    "    sns.barplot(data=roc_data, x='ROC AUC', y='Model', ax=axes[1,0])\n",
    "    axes[1,0].set_title('ROC AUC Comparison')\n",
    "\n",
    "# Training time comparison\n",
    "sns.barplot(data=ensemble_comparison_df, x='Training Time (s)', y='Model', ax=axes[1,1])\n",
    "axes[1,1].set_title('Training Time Comparison')\n",
    "axes[1,1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd407cd8",
   "metadata": {},
   "source": [
    "## ROC and Precision-Recall Curves for Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587694b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC and PR curves for ensemble methods\n",
    "prob_ensembles = [result for result in ensemble_results if result['y_pred_proba'] is not None]\n",
    "\n",
    "if prob_ensembles:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # ROC Curves\n",
    "    for result in prob_ensembles:\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "        auc_score = roc_auc_score(y_test, result['y_pred_proba'])\n",
    "        ax1.plot(fpr, tpr, label=f\"{result['model_name']} (AUC: {auc_score:.3f})\")\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curves - Ensemble Methods')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curves\n",
    "    for result in prob_ensembles:\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, result['y_pred_proba'])\n",
    "        ax2.plot(recall_curve, precision_curve, label=result['model_name'])\n",
    "    \n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curves - Ensemble Methods')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a145574",
   "metadata": {},
   "source": [
    "## Compare with Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143999e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load individual models comparison if available\n",
    "try:\n",
    "    individual_comparison_df = pd.read_csv(models_dir / 'individual_models_comparison.csv')\n",
    "    \n",
    "    # Combine individual and ensemble results\n",
    "    combined_comparison = pd.concat([individual_comparison_df, ensemble_comparison_df], ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"INDIVIDUAL vs ENSEMBLE MODELS COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Sort by F1 Score\n",
    "    combined_comparison_sorted = combined_comparison.sort_values('F1 Score', ascending=False)\n",
    "    print(combined_comparison_sorted.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Find overall best model\n",
    "    overall_best_idx = combined_comparison_sorted.iloc[0]\n",
    "    print(f\"\\nOverall Best Model: {overall_best_idx['Model']} (F1 Score: {overall_best_idx['F1 Score']:.4f})\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # F1 Score comparison - top 10\n",
    "    top_10 = combined_comparison_sorted.head(10)\n",
    "    sns.barplot(data=top_10, x='F1 Score', y='Model', ax=axes[0])\n",
    "    axes[0].set_title('Top 10 Models - F1 Score Comparison')\n",
    "    \n",
    "    # Model type comparison (Individual vs Ensemble)\n",
    "    individual_models = ['KNN', 'Optimized KNN', 'SVM', 'Optimized SVM', \n",
    "                        'Decision Tree', 'Optimized Decision Tree', \n",
    "                        'Random Forest', 'Optimized Random Forest']\n",
    "    \n",
    "    combined_comparison['Type'] = combined_comparison['Model'].apply(\n",
    "        lambda x: 'Individual' if x in individual_models else 'Ensemble'\n",
    "    )\n",
    "    \n",
    "    type_comparison = combined_comparison.groupby('Type')[['Accuracy', 'Precision', 'Recall', 'F1 Score']].mean()\n",
    "    \n",
    "    type_comparison.plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title('Individual vs Ensemble Models - Average Performance')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Individual models comparison file not found. Showing ensemble results only.\")\n",
    "    combined_comparison = ensemble_comparison_df\n",
    "    overall_best_idx = combined_comparison.loc[combined_comparison['F1 Score'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26362ab",
   "metadata": {},
   "source": [
    "## Save Best Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ded97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble models\n",
    "ensemble_models = {\n",
    "    'hard_voting': hard_voting_results['model'],\n",
    "    'soft_voting': soft_voting_results['model'],\n",
    "    'stacking_lr': stacking_results['model'],\n",
    "    'stacking_rf': stacking_rf_results['model'],\n",
    "    'ada_boost': ada_boost_results['model'],\n",
    "    'gradient_boost': gb_results['model'],\n",
    "    'weighted_voting': weighted_voting_results['model']\n",
    "}\n",
    "\n",
    "for model_name, model in ensemble_models.items():\n",
    "    model_path = models_dir / f\"{model_name}.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Saved {model_name} to {model_path}\")\n",
    "\n",
    "# Save ensemble comparison results\n",
    "ensemble_comparison_df.to_csv(models_dir / 'ensemble_models_comparison.csv', index=False)\n",
    "print(f\"\\nEnsemble comparison results saved to {models_dir / 'ensemble_models_comparison.csv'}\")\n",
    "\n",
    "# Save combined comparison if available\n",
    "if 'combined_comparison' in locals():\n",
    "    combined_comparison.to_csv(models_dir / 'all_models_comparison.csv', index=False)\n",
    "    print(f\"Combined comparison results saved to {models_dir / 'all_models_comparison.csv'}\")\n",
    "\n",
    "print(\"\\nAll ensemble models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d28b6",
   "metadata": {},
   "source": [
    "## Ensemble Methods Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d280f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final ensemble summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE METHODS TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. ENSEMBLE METHODS IMPLEMENTED:\")\n",
    "print(f\"   ✓ Hard Voting Classifier\")\n",
    "print(f\"   ✓ Soft Voting Classifier\")\n",
    "print(f\"   ✓ Stacking Classifier (Logistic Regression)\")\n",
    "print(f\"   ✓ Stacking Classifier (Random Forest)\")\n",
    "print(f\"   ✓ AdaBoost Classifier\")\n",
    "print(f\"   ✓ Gradient Boosting Classifier\")\n",
    "print(f\"   ✓ Weighted Voting Classifier\")\n",
    "\n",
    "print(f\"\\n2. BEST ENSEMBLE MODEL:\")\n",
    "print(f\"   Model: {best_ensemble_name}\")\n",
    "print(f\"   F1 Score: {best_ensemble_f1:.4f}\")\n",
    "print(f\"   Accuracy: {ensemble_comparison_df.loc[best_ensemble_idx, 'Accuracy']:.4f}\")\n",
    "print(f\"   Precision: {ensemble_comparison_df.loc[best_ensemble_idx, 'Precision']:.4f}\")\n",
    "print(f\"   Recall: {ensemble_comparison_df.loc[best_ensemble_idx, 'Recall']:.4f}\")\n",
    "\n",
    "if 'combined_comparison' in locals():\n",
    "    print(f\"\\n3. OVERALL BEST MODEL (Individual + Ensemble):\")\n",
    "    print(f\"   Model: {overall_best_idx['Model']}\")\n",
    "    print(f\"   F1 Score: {overall_best_idx['F1 Score']:.4f}\")\n",
    "    print(f\"   Type: {'Individual' if overall_best_idx['Model'] in individual_models else 'Ensemble'}\")\n",
    "    \n",
    "    ensemble_improvement = (\n",
    "        ensemble_comparison_df['F1 Score'].mean() - \n",
    "        individual_comparison_df['F1 Score'].mean()\n",
    "    ) * 100\n",
    "    print(f\"\\n4. ENSEMBLE IMPROVEMENT:\")\n",
    "    print(f\"   Average F1 improvement: {ensemble_improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\n5. KEY INSIGHTS:\")\n",
    "best_precision_ens = ensemble_comparison_df.loc[ensemble_comparison_df['Precision'].idxmax()]\n",
    "best_recall_ens = ensemble_comparison_df.loc[ensemble_comparison_df['Recall'].idxmax()]\n",
    "fastest_ens = ensemble_comparison_df.loc[ensemble_comparison_df['Training Time (s)'].idxmin()]\n",
    "\n",
    "print(f\"   - Best Precision (Ensemble): {best_precision_ens['Model']} ({best_precision_ens['Precision']:.4f})\")\n",
    "print(f\"   - Best Recall (Ensemble): {best_recall_ens['Model']} ({best_recall_ens['Recall']:.4f})\")\n",
    "print(f\"   - Fastest Training: {fastest_ens['Model']} ({fastest_ens['Training Time (s)']:.3f}s)\")\n",
    "\n",
    "print(f\"\\n6. ENSEMBLE ADVANTAGES:\")\n",
    "print(f\"   - Reduced overfitting through model diversity\")\n",
    "print(f\"   - Improved generalization performance\")\n",
    "print(f\"   - More robust predictions\")\n",
    "print(f\"   - Better handling of model weaknesses\")\n",
    "\n",
    "print(f\"\\n7. NEXT STEPS:\")\n",
    "print(f\"   - Final model evaluation and selection\")\n",
    "print(f\"   - Business impact analysis\")\n",
    "print(f\"   - Model deployment recommendations\")\n",
    "print(f\"   - Performance monitoring setup\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE METHODS COMPLETED - Ready for final evaluation\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
